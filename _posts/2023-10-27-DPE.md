
---
title:  "DPE: Disentanglement of Pose and Expression for General Video Portrait Editing"

categories:
  - Paper/One-shot video-driven_talking_face_generation

toc: true
toc_sticky: true
 
date: 2023-10-27 
last_modified_at: 2023-10-27

---

## Intro
- Top: disentanglement of pose and expression. Bottom: general video editing![](../source/ìŠ¤í¬ë¦°ìƒ·%202023-10-27%20ì˜¤ì „%209.15.20.png)
- â€œOne-shot video-driven talking face generation, using novel self-supervised disentanglement framework to decouple pose and expression without 3DMMs and paired data.â€
- â€œHead pose and facial expression are always entangled in facial motion and transferred simultaneously.â€
- â€œOne challenge of decoupling pose and expression is the lack of paired data, such as the same pose but different expressions. Only a few methods attempt to tackle this challenge with the feat of 3D Morphable Models (3DMMs) for explicit disentanglement. But 3DMMs are not accurate enough to capture facial details due to the limited number of Blenshapes, which has side effects on motion transfer.â€

- â€œThe editing module projects faces into a latent space where pose motion and expression motion can be disentangled, and the pose or expression transfer can be performed in the latent space conveniently via addition. The two generators render the modified latent codes to images, respectively. Moreover, to guarantee the disentanglement, we propose a bidirectional cyclic training strategy with well-designed constraints.â€

- â€œOur model contains three learnable components, i.e., the motion editing module, the expression generator, and the pose generator.â€
## Method
- â€œThese two generators share the same architecture but different parameters for interpreting the pose and expression code respectively.â€
- â€œInspired by the flow-based methods [24, 25], we use flow fields to manipulate the feature maps. Fig. 2 gives an illustration of the generators. Similar to the pipeline of StyleGAN2 [16], we exploit the latent code to generate multiscale flow fields that are used to warp the feature maps from the encoder in the motion editing module.â€![](../source/Pasted%20image%2020231028002531.png)
- â€œAs shown in Fig. 3, the three dash lines indicate that three pairs of images can be used to compute reconstruction losses, i.e., < Sâ€²â€², D >, < Dâ€²â€², S >, and < Sâ€², Dâ€² >.â€

- â€œFortunately, we discover that the self-reconstruction of the two generators is core for the disentanglement, i.e., the pair < S, e(S, S) > and < S, p(S, S) >.â€![](../source/Pasted%20image%2020231028002544.png)
- **Loss functions used for training modules** ![](../source/Pasted%20image%2020231028002606.png)![](../source/Pasted%20image%2020231028002620.png)![](../source/Pasted%20image%2020231028002625.png)![](../source/Pasted%20image%2020231028002630.png)
## Experiment
- **Quan**![](../source/Pasted%20image%2020231028002659.png)
- **Qual**![](../source/Pasted%20image%2020231028002703.png)
![](../source/Pasted%20image%2020231028002612.png)![](../source/Pasted%20image%2020231028002715.png)
## Conclusion from ğŸ¦–
- 