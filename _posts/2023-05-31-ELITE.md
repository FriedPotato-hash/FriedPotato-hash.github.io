
### Encoding Visual Concepts into Textual Embeddings for Customized Text-to-Image Generation

created at : 2023-05-31 21:15
#arxiv23_Feb #CLIPspace #learning-based #encoder

## Intro
- leaning-based encoder 사용하여 concept customization을 정확하고 빠르게 진행함. 이는 global, local mapping network로 구성된다. local mapping network는 cross attn layer에 detail한 object의 정보를 넣어주며, patch feature 뽑아서 넣음.
![[Pasted image 20230531211758.png]]

## Method
- Image Encoder는 CLIP image encoder (pretrained)를 사용하고 CLIP intermediate layer가 각각 hierarchical feature를 뽑아내므로 가장 깊은 layer에서의 feature를 사용함. ![[Pasted image 20230531212156.png]]
- 실제 사용되는건 primary word 하나뿐. 다른 word embedding들은 결국 auxillary information들을 없애주기 위해 존재
- loss는 다음과 같이 줘서 global mapping 을 학습시킴.![[Pasted image 20230531212223.png]]
- inference는 다음과 같이 해준다. ![[Pasted image 20230531212609.png]]

## Experiment
- CLIP image encoder에서 layer의 위치에 따른 feature를 visualization해본 결과. deepest layer에서 뽑아내는 결과가 보통 general한 concept을 담고 있다.
- "A photo of a [w_i]"라는 text prompt로 w_i는 ith layer의 CLIP image encoder의 output embedding.![[Pasted image 20230531212302.png]]


## Conclusion from 🦖
- InstantBooth에게 큰 영향을 준 논문일 듯.
- Image patch feature를 CLIP encoder를 통해 뽑아서 얘를 cross attn에 넣어주면 fine detail을 잡아주기에 더 좋다고 하네요. 
- 우리 task에 사용하기 위해서는 UMM-Diffusion 방식과 혼합해서 사용하면 좋을듯.