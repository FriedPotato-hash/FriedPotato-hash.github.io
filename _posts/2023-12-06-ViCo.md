
---
title:  "ViCo: Detail-Preserving Visual Condition for Personalized Text-to-Image Generation"

categories:
  - Paper/Personalization

toc: true
toc_sticky: true
 
date: 2023-12-06 
last_modified_at: 2023-12-06

---
## Intro
- capture fine visual details of the novel concept and generate photorealistic images following a text condition![|500](../source/Ïä§ÌÅ¨Î¶∞ÏÉ∑%202023-12-06%20Ïò§Ï†Ñ%2011.05.04.png)
- plug-in method
- propose an image attention module to condition the diffusion process on the patch-wise visual semantics
- attention-based object mask that comes almost at no cost from the attention module.
- a simple regularization based on the intrinsic properties of text-image attention maps to alleviate the common overfitting degradation
- Unlike many existing models, our method does not finetune any parameters of the original diffusion model. This allows more flexible and transferable model deployment. With only light parameter training (‚àº6% of the diffusion U-Net)
- DreamBooth and Custom Diffusion both meet the issue of language drift [26, 27] because finetuning the pretrained model on new data can lead to a loss of the preformed language knowledge. They leverage a preservation loss to address this problem, which requires manually generating [41] or retrieving massive class-specific images.![|700](../source/Ïä§ÌÅ¨Î¶∞ÏÉ∑%202023-12-06%20Ïò§Ï†Ñ%2011.14.51.png)

## Method
### Visual condition injection
- Instead of only considering the patches at the same spatial location on the noisy latent code and visual condition, we exploit correlations across all patches on both images. To this end, we propose to train an image cross-attention block that has the same structure as the text cross-attention block in the vanilla diffusion U-Net.![](../source/Ïä§ÌÅ¨Î¶∞ÏÉ∑%202023-12-06%20Ïò§Ï†Ñ%2011.15.42.png)
- Encoder-based tuning, InstantBooth acquire visual conditions from reference images by additionally training a visual feature extractor.
- To prevent the common overfitting problem, we introduce a simple yet effective regularization between the cross-attention maps associated with the end-of-text token and the learnable token. This regularization can be easily deployed without requiring any heavy preprocessing steps like image generation [41] and retrieval [23].
## Experiment
- 

## Conclusion from ü¶ñ
- 