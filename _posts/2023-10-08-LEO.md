
---
title:  "LEO: Generative Latent Image Animator for Human Video Synthesis"
excerpt: "Human video synthesis with disentanglement of motion and appearance"

categories:
  - Paper/video
tags:
  - [video_generation, motion_appearance_disentangle]

toc: true
toc_sticky: true
 
date: 2023-10-08 
last_modified_at: 2023-10-08

---
#motion_generation_diffusion
#LIA

## Intro
- It can generate unconditional video, conditional video which is based on one single image and video editing from the starting image.
- Spatio-temporal conherency is very importance feature for generating videos. To resolve this challenge, previous approaches have resorted to different features in the generation process aimed at representing appearance and motion. But their results were not good enough, resulting in spatial distortions and temporal jittering that break the spatio-temporal coherency.
- **LEO**'s idea is to represent motion as a sequence of flow maps in the generation process, which inherently isolate motion from appearance. We implement this idea via a flow-based image animator and a Latent Motion Diffusion Model (LMDM). 

## Method
- LEO imply two-stage training approach.
- “starting motion code α1 and a sequence of noise-vectors as input” (Wang 등, 2023, p. 2)
“Given such initial frame and a sequence of motion codes sampled from the LMDM, the flow-based image animator generates a sequence of flow maps, and proceeds to synthesize the corresponding sequence of frames in a warpand-inpaint manner.” (Wang 등, 2023, p. 2)
“The training of LEO is decomposed into two phases. Firstly, we train the flow-based image animator to encode input images into low-dimensional latent motion codes, and map such codes to flow maps, which are used for reconstruction via warp-and-inpaint. Therefore, once trained, the flow-based image animator naturally provides a space of motion codes that are strictly constrained to only containing motion-related information. At the second stage, upon the space provided by the image animator, we train the LMDM to synthesize sequences of motion codes and capture motion prior in the training data. To endow LEO with the ability to synthesize videos of arbitrary length beyond the short training videos, we adopt a Linear Motion Condition (LMC) mechanism in LMDM.” (Wang 등, 2023, p. 2)
“synthesize sequences of residuals w.r.t. a starting motion code, in order for longer videos to be easily obtained by concatenating additional sequences of residuals.” (Wang 등, 2023, p. 3)
![](../source/Pasted%20image%2020231008191205.png)
- This is the inference process figure.
![](../source/Pasted%20image%2020231008191124.png)
- In short, LEO's 2-stage training process can be summarized as below;
“In the first phase, we train the image animator in a self-supervised manner towards mapping latent codes to corresponding flow maps φi. Once the image animator is well-trained, motion codes a are extracted from a frozen encoder and used as inputs of LMDM.” (Wang 등, 2023, p. 4)
“In the second phase, LMDMs are trained to learn the motion distribution by providing the starting motion α1 as condition. Instead of directly learning the distribution of a, we adopt a Linear Motion Condition (LMC) mechanism in LMDM towards synthesizing sequences of residuals with respect to x1.” (Wang 등, 2023, p. 4)
## Experiment
- Quantitative results![|](../source/스크린샷%202023-10-08%20오후%207.14.17.png)
![|500](../source/스크린샷%202023-10-08%20오후%207.14.22.png)
![|300](../source/스크린샷%202023-10-08%20오후%207.14.30.png)
- Qualitative results![|400](../source/스크린샷%202023-10-08%20오후%207.13.59.png)
## Conclusion from 🦖
- This paper incororated a Latent Image Animator (LIA), as well as a Latent Motion Diffusion Model.
- Novelty is not enough