
---
title:  "EDGE: Editable Dance Generation From Music"
excerpt: ""

categories:
  - Paper/motion_generation
tags:
  - [Dance_generation, Diffusion]

toc: true
toc_sticky: true
 
date: 2023-10-19 
last_modified_at: 2023-10-19

---
#CVPR23  

## Intro
- Editable Dance GEneration (EDGE), a state-of-the-art method for editable dance generation that is capable of creating realistic, physically-plausible dances while remaining faithful to the input music.
- EDGE uses a transformer-based diffusion model paired with Jukebox, a strong music feature extractor, and confers powerful editing capabilities well-suited to dance, including joint-wise conditioning, and in-betweening.
- We introduce a new metric for physical plausibility, and evaluate dance quality generated by our method extensively through (1) multiple quantitative metrics on physical plausibility, beat alignment, and diversity benchmarks, and more importantly, (2) a large-scale user study, demonstrating a significant improvement over previous state-of-the-art methods.

## Method
- **EDGE pipeline Overview**, EDGE learns to denoise dance sequences from time t = T to t = 0, conditioned on music. Music embedding information is provided by a frozen Jukebox model [5] and acts as cross-attention context. EDGE takes a noisy sequence zT ∼ N (0, I) and produces the estimated final sequence xˆ, noising it back to zˆT −1 and repeating until t = 0.![](../source/스크린샷%202023-10-19%20오후%205.01.01.png)
- **Pose representation**, sequences of poses in the 24-joint SMPL format [34], using the 6- DOF rotation representation [62] for every joint and a single root translation: w ∈ R 24·6+3=147. For the heel and toe of each foot, we also include a binary contact label: b ∈ {0, 1} 2·2=4. The total pose representation is therefore x = {b, w} ∈ R 4+147=151 . EDGE uses a diffusion-based framework to learn to synthesize sequences of N frames, x ∈ R N×151, given arbitrary music conditioning c.
- **Loss**, L1 loss + Auxiliary Losses(joint positions (Eq. (3)), velocities (Eq. (4)), and foot velocities via our Contact Consistency Loss (Eq. (5)).), ![](../source/스크린샷%202023-10-19%20오후%205.16.11.png)![](../source/스크린샷%202023-10-19%20오후%205.18.25.png)![](../source/스크린샷%202023-10-19%20오후%205.18.44.png)
- **Sampling - CFG**,![|400](../source/스크린샷%202023-10-19%20오후%205.21.03.png)
- **Editing**, To enable editing for dances generated by EDGE, they use the 'standard masked denoising technique'. EDGE supports any combination of temporal and joint-wise constraints, shown in Fig. 4. Given a joint-wise and/or temporal constraint x known with positions indicated by a binary mask m, we perform the following at every denoising timestep.![](../source/스크린샷%202023-10-19%20오후%207.51.07.png)For example, a user can perform motion in-betweening by providing a reference motion x known ∈ R N×151 and a mask m ∈ {0, 1} N×151, where m is all 1’s in the first and last n frames and 0 everywhere else. This would result in a sequence N frames long, where the first and last n frames are provided by the reference motion and the rest is filled in with a plausible “in-between” dance that smoothly connects the constraint frames, for arbitrary 2n < N. This editing framework provides a powerful tool for downstream applications, enabling the generation of dances that precisely conform to arbitrary constraints.![|400](../source/스크린샷%202023-10-19%20오후%207.54.32.png)
- **Long video generation**, EDGE is trained on 5-second clips, it can generate choreographies of any length by imposing temporal constraints on batches of sequences. Below examples, Edge constains the first 2.5 seconds of each sequence to match the last 2.5 seconds of the previous one to generate a 12.5-second clip.![|400](../source/스크린샷%202023-10-19%20오후%207.47.23.png)
## Experiment
- **Table 1.** Comparison between FACT and Bailando.![](../source/스크린샷%202023-10-19%20오후%207.55.44.png)
- **Table2**. Ablation Study![](../source/스크린샷%202023-10-19%20오후%207.56.40.png)
## Conclusion from 🦖
- 