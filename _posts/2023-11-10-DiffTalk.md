
---
title:  "DiffTalk: Crafting Diffusion Models for Generalized Audio-Driven Portraits Animation"

---
#CVPR23 

## Intro
- Recently, a lot of effort has been devoted in this research area to improve the generation quality or enhance the model generalization. However, there are few works able to address both issues simultaneously, which is essential for practical applications.

## Method
- Overview of the proposed DiffTalk for generalized talking head synthesis. Apart from the audio condition to drive the lip motions, we further incorporate reference images and facial landmarks as extra driving factors for personalized facial modeling. In this way, the generation process is more controllable, which enables the learned model to generalize across different identities without further fine-tuning. Furthermore, we can gracefully improve our DiffTalk for higher-resolution synthesis with slight extra computational cost.![](../source/스크린샷%202023-11-10%20오후%208.01.42.png)![](../source/스크린샷%202023-11-10%20오후%208.02.01.png)![](../source/스크린샷%202023-11-10%20오후%208.02.11.png)
## Experiment
- ![](../source/Pasted%20image%2020231110200242.png)![](../source/스크린샷%202023-11-10%20오후%208.02.21.png)![](../source/스크린샷%202023-11-10%20오후%208.02.29.png)

## Conclusion from 🦖
- 