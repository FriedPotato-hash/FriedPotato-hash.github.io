
---
title:  "Conditional Image-to-Video Generation with Latent Flow Diffusion Models"

toc: true
toc_sticky: true
 
date: 2023-10-30 
last_modified_at: 2023-10-30

---
#CVPR23 

## Intro
- Conditional image-to-video (cI2V) generation aims to synthesize a new plausible video starting from an image (e.g., a person’s face) and a condition (e.g., an action class label like smile).![](../source/Pasted%20image%2020231030205720.png)
- Novel latent flow diffusion models (LFDM) that synthesize an optical flow sequence in the latent space based on the given condition to warp the given image.
- The training of LFDM consists of two separate stages: (1) an unsupervised learning stage to train a latent flow auto-encoder for spatial content generation, including a flow predictor to estimate latent flow between pairs of video frames, and (2) a conditional learning stage to train a 3D-UNet-based diffusion model (DM) for temporal latent flow generation.
## Method
- **Overall Architecture)** ![](../source/스크린샷%202023-10-30%20오후%209.09.12.png)
### Training
- `In stage one`, inspired by recent motion transfer works [50,62,75,76,79], a latent flow autoencoder (LFAE) is trained in an unsupervised fashion. It first estimates the latent optical flow between two frames from the same video, a reference frame and a driving frame. Then the reference frame is warped with predicted flow and LFAE is trained by minimizing the reconstruction loss between this warped frame and the driving frame. 
	- Here, m contains values changing from 0 to 1 to indicate the degree of occlusion, where 1 is not occluded and 0 mean s entirely occluded. The final warped latent map $z$ can be produced by:![](../source/스크린샷%202023-10-30%20오후%209.26.01.png)
	- Decoder inpaints the occluded parts of latent z for generating output image $\hat{x}_{out}$.![|300](../source/스크린샷%202023-10-30%20오후%209.28.35.png)
- `In stage two`, a 3D U-Net [12] based diffusion model (DM) is trained using **paired condition y** and **latent flow sequence extracted from training videos using trained LFAE**. Conditioned on y and image $x_0$, the DM learns how to produce temporally coherent latent flow sequences through 3D convolutions.![](../source/스크린샷%202023-10-30%20오후%209.32.49.png)
### Inference process of LFDM 
- The generated flow sequence $\hat{f}^K_1$ and occlusion map sequence $\hat{m}^K_1$ have the same spatial size as image latent map $z_0$. The brighter regions in $\hat{m}^K_1$ mean those are regions less likely to be occluded.![](../source/Pasted%20image%2020231030210602.png)
- 
## Experiment
- Qualitative comparison among different methods on multiple datasets for cI2V generation.![](../source/Pasted%20image%2020231030210806.png)
- Quantitative comparison among different methods on multiple datasets for cI2V generation![](../source/스크린샷%202023-10-30%20오후%209.08.24.png)
## Conclusion from 🦖
- 