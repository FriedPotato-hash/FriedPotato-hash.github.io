### Exemplar-based Image Editing with Diffusion Models

created at : 2023-06-03 17:48
#arxiv22_Nov  

## Intro
- conditional imageë¡œ sceneì„ editingí•˜ëŠ” íƒœìŠ¤íŠ¸. reference imageì˜ ê°ì²´ë¥¼, source imageì˜Â  editing region (masking)ì— ë„£ìŒ. ê¸°ë³¸ì ìœ¼ë¡œ blended diffusion, shape guided diffusionê³¼ ê°™ì€ ë¬¸ì œë¥¼ í’€ì–´ë‚´ëŠ”ë° ë§ˆìŠ¤í‚¹ëœ ìœ„ì¹˜ì— ì±„ì›Œë„£ì–´ì•¼í•  ì´ë¯¸ì§€ guidanceë¥¼ textê°€ ì•„ë‹Œ imageë¡œ ì œê³µí•œë‹¤ëŠ”ë° ìˆë‹¤.![[ìŠ¤í¬ë¦°ìƒ· 2023-06-03 ì˜¤í›„ 5.50.01.png]]
- self-supervised trainingì„ leveragingí•´ì„œ source imageë‘ exemplarë¥¼ disentangle&re-organize í•  ìˆ˜ ìˆê²Œ ë¨! 
- naive approachëŠ” fusing artifactsë¥¼ ìœ ë°œí•  ìˆ˜ ìˆìœ¼ë‹ˆ ì´ì— ê´€ë ¨í•˜ì—¬ ë¶„ì„ì„ ì¶”ê°€í–ˆê³ , information bottleneckì„ ì œì•ˆí–ˆìœ¼ë©° exemplarë¥¼ ê·¸ëƒ¥ copy&pasteí•˜ëŠ” trivial solutionì„ ë°©ì§€í•˜ê¸° ìœ„í•´ augmentation ë°©ë²•ì„ ì¶”ê°€í•¨.
- iterative optimizationì—†ê³ , diffusion modelì˜ one-forwardë¡œ ì´ë¯¸ì§€ ìƒì„±í•´ëƒ„.

## Method
- í•´ê²°í•´ì•¼ë˜ëŠ”ë¬¸ì œ  
	1) referenceì˜ examplerì˜ semanticí•œ ì •ë³´ì™€ fine detailì„ ë”°ì™€ì•¼ í•˜ë©° ê·¸ ë„ì¤‘ì— backgroundë¥¼ ë†“ì¹˜ë©´ ì•ˆë¨  
	2) examplerë¥¼ ì ì ˆí•˜ê²Œ trasnformí•´ì„œ ë„£ì–´ì£¼ëŠ”ê²Œ ì—¬ê°„ ì‰¬ìš´ ì¼ì´ ì•„ë‹˜.  
	3) merging boundaryê°€ ì–´ìƒ‰í•˜ë©´ ì•ˆë¨  
	4) reference ì´ë¯¸ì§€ì˜ resolutionì´ source imageë³´ë‹¤ ì•ˆì¢‹ì„ë•Œê°€ ë§ìŒ. ë”°ë¼ì„œ processê³¼ì •ì— super-resolutionì„ í¬í•¨í•´ì•¼ ë¨.
- conditionìœ¼ë¡œÂ  CLIP textencoderì˜ ê²°ê³¼ (77í† í°)ì„ ë„£ì–´ì£¼ëŠ”ë° ì–˜ë¥¼ CLIP image encoderì˜ ê²°ê³¼ë¡œ ë‹¨ìˆœ ëŒ€ì²´í•´ì„œ ë„£ì–´ì¤€ ê²°ê³¼.  training ê³¼ì •ì—ì„œ lossì˜ ë–¨ì–´ì§€ëŠ”ë° test set ëŒë ¤ë³´ë©´ unsatisfactí•˜ë‹¤.![[Pasted image 20230603175222.png]]
- training process![[Pasted image 20230603175256.png]]
- CLIP text tokenì„ conditionìœ¼ë¡œ ì‚¬ìš©í•œë‹¤ë©´ semanticí•œ ì˜ë¯¸ë¥¼ intrincsicí•˜ê²Œ í•´ì„í•  ìˆ˜ ìˆê²Œ ëœë‹¤. ê·¸ëŸ¬ë‚˜ CLIP image tokenì€ imageì— ëŒ€í•œ ì •ë³´ë¥¼ ê·¸ìì²´ë¡œë„ ë„ˆë¬´ ì˜ ìœ ì§€í•˜ê³  ì‡ì–´ì„œ copy/pasteë¼ëŠ” trivial solutionìœ¼ë¡œ ê°€ê¸°ê°€ ì‰¬ìš´ê²ƒ...
- í•´ë‹¹ ë…¼ë¬¸ì—ì„œëŠ” ê·¸ë˜ì„œ one dimensional 1024 vector - class tokenë§Œì„ ì‚¬ìš©í•œë‹¤!!!  high frequency detailì€ ì¢€ ë²„ë ¤ì£¼ëŠ” ë™ì‹œì—, highly compressedëœ semantic informationì„ ì œê³µí•œë‹¤.  
- null text conditionì„ ì£¼ë“¯ learnable vector vë¥¼ 20%ì˜ ë¹„ìœ¨ë¡œ reference ocnditionì„ ëŒ€ì²´í•´ì„œ ë¶€ì—¬í•¨. inference stepì—ì„œëŠ” CFG ì‹ì„ ë‹¤ìŒê³¼ ê°™ì´ ì§œë‘ . ![[Pasted image 20230603175409.png]]

## Experiment
- Dataset
	- Then we select OpenImages [32] as our training dataset. It contains a total of 16 million bounding boxes for 600 object classes on 1.9 million images. 
- Resources
	- During training, we preprocess the image resolution to 512Ã—512, and train our model for 40 epochs, which takes about 7 days on 64 NVIDIA V100 GPUs.
- Comparisons ![[Pasted image 20230603175500.png]]
- Ablations![[Pasted image 20230603175511.png]]
- Cute results..![[Pasted image 20230603175522.png]]

## Conclusion from ğŸ¦–
- fine detailì„ ì¡ëŠ”ë° CLIP image encoderê°€ í™•ì‹¤íˆ ë„ì›€ì´ ë˜ê² ë‹¤ ì‹¶ë‹¤.
- ë‚˜ì˜¨ì§€ ê½¤ ì‹œê°„ì´ ì§€ë‚œ ë…¼ë¬¸ì¸ë° demo ê²°ê³¼ë„ ë‚˜ì˜ì§€ ì•Šê³  ì¬ë°Œì—ˆìŒ. ë°ëª¨ ëŒë ¤ë³¸ ê²°ê³¼ê°€ ì¡°ê¸ˆ ì•„ì‰¬ì› ë˜ ê²ƒì€, examplarì—ì„œ high-levelì˜ semanticí•œ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ê²½í–¥ì´ ë” ì»¸ë‹¤ëŠ” ê²ƒ. ì˜ˆë¥¼ ë“¤ì–´ ê±´ë‹´ ì´ë¯¸ì§€ë¥¼ ë„£ê³  ì‹¶ì€ë° ì™  ë¿ŒìŠíŒŒìŠ ë²ˆì©ë²ˆì©í•œ ë¡œë´‡ì„ ê°–ë‹¤ ë†“ì•˜ë‹¤. 
- resource ì†Œìš”ê°€ í¬ë‹¤ëŠ” ê²ƒë„ ì•„ì‰¬ìš´ ì . 