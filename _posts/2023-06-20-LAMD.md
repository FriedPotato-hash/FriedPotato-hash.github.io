### Latent Motion Diffusion for Video Generation
created at : 2023-06-20 21:54
#arxiv23_Apr  #image-to-video #text-image-to-video

## Intro
- video frames로 하여금 제일 첫번째 frame을 content로 하고, motion을 따로 encoding함. 즉, 비디오에서 content와 motion을 decompose한다.![[Pasted image 20230620215530.png]]

## Method
- **2 stage training**; 
	- MCD-VAE를 video-only data로 다음 loss를 통해 training ![[Pasted image 20230620220139.png|300]]
	- parameter를 고정해서 DMG(denoising motion generator)를 학습함. recon loss 사용, y는 content feature와 optional condition(text)로 구성된다. ![[Pasted image 20230620220206.png|300]]
- **Sampling**; 
	- DMG가 학습한 dataset의 motion distribution 기반으로, content feature 혹은 text와 같은 optional condition을 기반으로 motion latent를 생성해냄. 
- **Overall Architecture**![[스크린샷 2023-06-20 오후 9.59.28.png]]
- decoder는 3D Unet 기반 모델이고, spatial-aligned motion latents를 fusing 한 다음, gradually incorporating multi-scale content feature 한다. (skip connections으로)
- Sampling process 비교. 기존 video diffusion 모델들에 비해 훨씬 cost-efficient하다.![[Pasted image 20230620220234.png|400]]

## Experiment
- MCD-VAE reconstruction 결과![[Pasted image 20230620220749.png|400]]
- content와 motion에 대해 다른 data를 사용해서 video를 generation 하는 것![[Pasted image 20230620220812.png|400]]
- multi-modal condition(text)을 같이 준 경우 ![[Pasted image 20230620220907.png]]

## Conclusion from 🦖
- MCD-VAE로 하여금 content와 motion을 decompose하도록 학습시키는데 KL-divergence가 3D CNN encoder의 결과로 하여금 content 정보를 짜낸다는게 새로 알게 된 사실. Adain에서 mean, variance를 주입하면 image의 content가 아닌 style이 반영되는 것과 유사한 원리이지 않나?
- 모델이 간단한데, cost-efficient한 방법이라 놀라웠음. VAE 구조는 video recon quality를 책임지고, motion distribution 학습은 diffusion model로 하여금 하도록 함. PVDM 구조를 생각해보면, projection된 3개의 plane이 content를 담으면서도 motion에 대한 정보를 (손실이 있긴 하지만) 포함한다는 점에서 VAE 역할이 같은 결이라고 생각됨.
- 마찬가지로 diffusion 모델은 dataset에서의 motion의 distribution을 잘 모델링 해주는 역할을 한다.