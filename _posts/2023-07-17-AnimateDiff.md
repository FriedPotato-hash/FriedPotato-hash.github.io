### Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning

created at : 2023-07-17 10:15
#arxiv23_Jul 

## Intro
- Without specific tuning, most personalized T2I models could be directly animated by inserting the well-trained motion modeling module.
- hugging face dreambooth weights, CIVIT ai weightsë¥¼ ê·¸ëŒ€ë¡œ ê°€ì ¸ë‹¤ê°€ motion module weightë§Œ ê°ˆì•„ ë¼ì›Œì¤˜ì„œ generalí•˜ê²Œ ì ìš©í•  ìˆ˜ ìˆëŠ” ëª¨ë¸ì„ ë§Œë“¤ì—ˆë‹¤ëŠ” ê²ƒì´ ì¥ì 

## Method
- motion modeling moduleì„ video datasetì— ëŒ€í•´ training í•´ì£¼ê³ , motion priorê°€ distilë  ìˆ˜ ìˆë„ë¡ í•¨. T2I base parameterëŠ” frozen ì‹œì¼œì„œ feature spaceë¥¼ ìœ ì§€í•  ìˆ˜ ìˆë„ë¡ í•¨.![[ìŠ¤í¬ë¦°ìƒ· 2023-07-17 ì˜¤ì „ 10.18.02.png]]
- temporal and spatial axes are reshaped into the batch axis separately. motion moduleì€ vanilla temporal transformerì´ê³ , zero-initialized output project layerê°€ ë‹¬ë ¤ìˆìŒ![[ìŠ¤í¬ë¦°ìƒ· 2023-07-17 ì˜¤ì „ 10.21.09.png]]
- loss ì‹ì€ latent diffusion ì´ë‘ ë¹„ìŠ·.![[ìŠ¤í¬ë¦°ìƒ· 2023-07-17 ì˜¤ì „ 10.28.23.png|400]]

## Experiment
- 256x256, 16 frames video clip. 
- During experiments, we discovered that using a diffusion schedule slightly different from the original schedule where the base T2I model was trained helps achieve better visual quality and avoid artifacts such as low saturability and flickering. Thus, we used a linear beta sched- ule, where Î²start = 0.00085 and Î²end = 0.012.
- **Text2Video-zero vs. AnimateDiff** - It is noticeable that while the baseline results lack fine-grain consistency, our method maintains a better temporal smoothness.![[ìŠ¤í¬ë¦°ìƒ· 2023-07-17 ì˜¤ì „ 10.32.47.png]]
## Conclusion from ğŸ¦–
- LVDM + Pose diffusion ì„ ì„ì€ ëŠë‚Œ.
- rerender a videoê°€ ë” ì˜ë  ê±° ê°™ë‹¤.