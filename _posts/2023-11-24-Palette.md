
---
title:  "Palette: Image-to-Image Diffusion Models"
 
date: 2023-11-24 
last_modified_at: 2023-11-24

---
### Abstract

<aside> ⛰️ 1. specific customization이 필요없는 general한 I2I model, Pallete 제안 2. L1/L2 loss가 denoising loss에 적용될 때 분석 3. self-attention Layer의 중요성을 실험으로 증명 4. ImageNet 기반으로 새로운 evaluation metric 제안

</aside>
![](../source/스크린샷%202023-11-24%20오후%209.05.24.png)

a simple and general framework for image-to-image translation using conditional diffusion models. I2I translation four tasks (colorization, inpainting, uncropping, and JPEG decompression). task-specific hyper-parameter tuning, architecture customization, or any auxiliary loss, demonstrating a desirable degree of generality and flexibility 없이 strong GAN/regression baselines 보다 좋은 성과를 냈다.

1. L2와 L1 loss를 비교도 해줌 (in the denoising diffusion objective on sample diversity)
2. demonstrate the importance of self-attention through empirical architecture studies
3. advocate a unified evaluation protocol based on ImageNet

---

### Introduction

![|400](../source/Pasted%20image%2020231124211120.png)
Many such tasks, like those in Figure [1](https://ar5iv.labs.arxiv.org/html/2111.05826#S0.F1), are complex inverse problems, where multiple output images are consistent with a single input.

A natural approach to image-to-image translation is to learn the conditional distribution of output images given the input using deep generative models that can capture multi-modal distributions in the high-dimensional space of images.

Nevertheless, GANs can be challenging to train (e.g., Arjovsky et al., [2017](https://ar5iv.labs.arxiv.org/html/2111.05826#bib.bib3); Gulrajani et al., [2017](https://ar5iv.labs.arxiv.org/html/2111.05826#bib.bib25)), and often drop modes in the output distribution.

This paper investigates the general applicability of _Palette_, our implementation of image-to-image diffusion models, to a suite of four distinct and challenging tasks, namely colorization, inpainting, uncropping (_a.k.a_ outpainting or extrapolation), and JPEG artifact removal.

With no task-specific architecture customization, and no changes to the hyper-parameters or the loss, Palettecan deliver high-fidelity outputs across all four tasks.

We find that while L2 and L1 losses in the denoising objective yield similar sample-quality scores, L2 leads to a higher degree of diversity in model samples, whereas L1 produces more conservative outputs.

We also find that removing self-attention layers from the U-Net architecture of Palette, in order to build a fully convolutional model, hurts performance.

**Contribution 정리**

1. Palette, implementation of I2I diffusion models
2. Recognize the impact of L1 vs L2 in the denoising objective on sample diversity
3. Show the importance of self-attention through empirical studies.
4. Propose a unified evalution protocol across image translation tasks based on the ImageNet dataset.

---

### Related works

**inpainting**

linear inverse tasks

**Image uncropping**

보통 inpainting보다 어려운 task로 여겨진다.

**Colorization**

scene understanding을 요구하는 task!

**JPEG decompression**

nonlinear inverse problem for restoring realistic textures and removing compression artifacts.

---

### PALLETE

**architecture;**

The network architecture is based on the 256×256 class-conditional U-Net model of [Dhariwal and Nichol 2021]. The two main differences between our architecture and theirs are (i) absence of class-conditioning, and (ii) additional conditioning of the source image via concatenation, following [Saharia et al. 2021].

**loss function;**
![](../source/스크린샷%202023-11-24%20오후%209.11.34.png)

---

### The Role of Self-Attention in Diffusion Model Architectures

The results in Table 6 thus clearly show that models trained with the 𝐿2 loss have greater sample diversity than those trained with the 𝐿1 loss.