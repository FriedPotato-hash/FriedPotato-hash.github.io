
---
title:  "StarGAN v2: Diverse Image Synthesis for Multiple Domains"

toc: true
toc_sticky: true
 
date: 2023-11-03
last_modified_at: 2023-11-03

---
#cvpr20

## Intro
- A good image-to-image translation model should learn a mapping between different visual domains while satisfying the following properties: 1) diversity of generated images and 2) scalability over multiple domains. Existing methods address either of the issues, having limited diversity or multiple models for all domains.![|500](../source/Ïä§ÌÅ¨Î¶∞ÏÉ∑%202023-11-04%20Ïò§Ï†Ñ%201.19.38.png)
- Previous i2i methods' domain-specific decoders interpret the latent codes as recipes for various styles when generating images. These methods have only considered a mapping between two domains, they are not scalable to the increasing number of domains. For example, having K domains, these methods require to train K(K-1) generators to handle translations between each and every domain, limiting their practical usage.
## Method
- StarGAN v2, a scalable approach that can generate diverse images across multiple domains. In particular, we start from StarGAN and replace its domain label with our proposed domainspecific style code that can represent diverse styles of a specific domain. To this end, we introduce two modules, a mapping network and a style encoder. The mapping network learns to transform random Gaussian noise into a style code, while the encoder learns to extract the style code from a given reference image.
- Architectures![|500](../source/Ïä§ÌÅ¨Î¶∞ÏÉ∑%202023-11-04%20Ïò§Ï†Ñ%201.35.49.png)
	- Generator (Figure 2a). Our generator G translates an input image x into an output image G(x, s) reflecting a domainspecific style code s, which is provided either by the mapping network F or by the style encoder E. We use adaptive instance normalization (AdaIN) [15, 22] to inject s into G. We observe that s is designed to represent a style of a specific domain y, which removes the necessity of providing y to G and allows G to synthesize images of all domains. 
	- Mapping network (Figure 2b). Given a latent code z and a domain y, our mapping network F generates a style code s = Fy(z), where Fy(¬∑) denotes an output of F corresponding to the domain y. F consists of an MLP with multiple output branches to provide style codes for all available domains. F can produce diverse style codes by sampling the latent vector z ‚àà Z and the domain y ‚àà Y randomly. Our multi-task architecture allows F to efficiently and effectively learn style representations of all domains. 
	- Style encoder (Figure 2c). Given an image x and its corresponding domain y, our encoder E extracts the style code s = Ey(x) of x. Here, Ey(¬∑) denotes the output of E corresponding to the domain y. Similar to F, our style encoder E benefits from the multi-task learning setup. E can produce diverse style codes using different reference images. This allows G to synthesize an output image reflecting the style s of a reference image x. 
	- Discriminator (Figure 2d). Our discriminator D is a multitask discriminator [30, 35], which consists of multiple output branches. Each branch Dy learns a binary classification determining whether an image x is a real image of its domain y or a fake image G(x, s) produced by G.

## Experiment
- Visual comparison of generated images![|400](../source/Ïä§ÌÅ¨Î¶∞ÏÉ∑%202023-11-04%20Ïò§Ï†Ñ%201.37.38.png)
- (a) The top three rows correspond to the results of converting male to female and vice versa in the bottom three rows. (b) Every two rows from the top show the synthesized images in the following order: cat-to-dog, dog-to-wildlife, and wildlife-to-cat.![](../source/Ïä§ÌÅ¨Î¶∞ÏÉ∑%202023-11-04%20Ïò§Ï†Ñ%201.39.53.png)
## Conclusion from ü¶ñ

