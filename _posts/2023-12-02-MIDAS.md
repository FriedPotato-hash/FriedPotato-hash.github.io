
---
title:  "Towards Robust Monocular Depth Estimation Mixing Datasets forZero-shot Cross-dataset Transfer"

toc: true
toc_sticky: true
 
date: 2023-12-02 
last_modified_at: 2023-12-02

---
### Abstract

- monocular depth estimation에는 large, diverse 훈련 데이터셋이 중요.
    
- Due to the **challenges** associated with acquiring dense ground-truth depth across different environments at scale, a number of datasets with distinct characteristics and biases have emerged. We develop tools that enable mixing multiple datasets during training, even if their annotations are incompatible.
    
- propose a robust training objective that is invariant to changes in depth range and scale, advocate the use of principled multi-objective learning to combine data from different sources, and highlight the importance of pretraining encoders on auxiliary tasks.
    
- To demonstrate the generalization power of our approach we use _zero-shot cross-dataset transfer._
    
    ⇒ mixing data from complementary sources greatly improves monocular depth estimation.
    

---

<aside> 🫒 Mixing data from complementary sources greatly improves monocular depth estimation.

</aside>

### Introduction

`**Existing Problems**`

- none of the existing datasets is sufficiently rich to support the training of a model that works robustly on real images of diverse scenes.
- 그러나 서로 보충이 될 만한 다양한 데이터셋이 다양하게 존재하는 상황.

**`Summary`**

- We develop novel loss functions that are invariant to the major sources of incompatibility between datasets, including unknown and inconsistent scale and baselines.
- Diverse sensing modalities들로부터 얻은 데이터(stereo cameras, laser scanners, structured light sensors)들까지도 학습 가능하도록 loss를 설계. 또한, monocular depth 모델 학습을 위한 데이터셋들을 어떻게 가장 이상적으로 혼합할지까지 연구했음. multi-objective optimization을 기반으로한 접근이 naive mixing 전략보다 성과가 좋았다.
- We further empirically highlight the importance of high-capacity encoders, and show the unreasonable effectiveness of pretraining the encoder on a large-scale auxiliary task.
- 6개 데이터셋에서 quantitatively & qualitatively하게 소타를 능가하는 성능 달성.
![](../source/Pasted%20image%2020231202002334.png)
- disparity & depth
    
![](../source/Pasted%20image%2020231202002327.png)
    

---

### Existing Datasets & 3D MOVIES
![](../source/Pasted%20image%2020231202002315.png)
![](../source/Pasted%20image%2020231202002309.png)
`**challenges**`

**1)**

3D 영화는 관객으로 하여금 편안한 시청을 제공하기 위해 artistic, psychophysical(정신물리학적)인 고려사항들 기반으로 disparity range를 결정하는 경우가 많다.

예를 영화의 시작부분과 끝부분에서는 disparity range가 커지지만, 영화의 중반부에서는 좀 더 편안한 시청을 위해 disparity range를 좁게 조절한다.

→ “focal lengths, baseline, convergence angle between the cameras of the stereo rig” 가 하나의 영화 안에서도 매우 달라지게 된다.

**2)**

보통의 stereo camera와는 다르게, 영화에서 사용되는 stereo pairs들은 positive, negative disparities를 포함한다. (screen보다 앞에 있는지 뒤에 있는지 표현하기 위한-)

**3)**

post-production으로 image pairs들을 shifting 처리하기도 함..

⇒ 위 문제들을 해결하기 위해 data extraction & training procedures.

`**Movie Selection and Preprocessing**`

- 23 편의 영화 선정, 다음과 같은 기준 적용
    
    1. We only selected movies that were shot using a physical stereo camera. (Some 3D films are shot with a monocular camera and the stereoscopic effect is added in post-production by artists.)
        
    2. We tried to balance realism and diversity.
        
    3. We only selected movies that are available in Blu-ray format and thus allow extraction of high-resolution images.
        
- 1920x1080 해상도, 24 fps(frames per second)로 추출 → 영화마다 장면 크기가 달라 black bar가 발생하는 경우가 있었음. 따라서 1880x800 사이즈로 center crop.
    

`**Disparity extraction**`

- stereo matching을 사용해서 pairs간의 disparity maps을 뽑아내려고 했지만, SOTA stereo matcher들도 movie data에서는 좋지 않은 결과를 보임. 이는 stereo matcher가 positive disparity ranges라는 가정하에서 학습되었기 때문. (standard stereo camera에게는 적합한 가정이지만, stereoscopic film에서는 들어맞지 않는다.)
- 기존의 데이터들에 비해, 3D 영화의 disparity range가 매우 작다(due to the limited depth budget)는 점도 문제.

⇒ optical flow algorithm을 적용해서 문제 해결.

- Optical flow algorithms naturally handle both positive and negative disparities and usually perform well for displacements of moderate size.
- stereo pair에서 left camera를 reference로 두고, optical flow를 추출. 이후에 right camera를 reference로 두고 optical flow를 다시 추출해 준 다음에, left-right consistency를 체크 → bad disparity quality를 갖는 frame은 filter out 가능!
- pretrained semantic segmentation 모델로 하늘 찾아내고, minimum disparity in the image로 세팅해준다.

---

### Training on Diverse Data

`**challenges**`

1. **Inherently different representations of depth**: direct vs. inverse depth representations.
    
2. **Scale ambiguity**: for some data sources, depth is only given up to an unknown scale.
    
3. **Shift ambiguity**: some datasets provide disparity only up to an unknown scale and global disparity shift that is a function of the unknown baseline and a horizontal shift of the principal points due to post-processing
    

→ The main requirement for a sensible training scheme is to carry out computations in an appropriate output space that is compatible with all ground-truth representations and is numerically well- behaved and to design a loss function that is flexible enough to handle diverse sources of data.

**`Scale- and shift-invariant losses.`**

- We propose to perform pre- diction in disparity space (inverse depth up to scale and shift) together with a family of scale- and shift-invariant dense losses to handle the aforementioned ambiguities.
![](../source/Pasted%20image%2020231202002249.png)
- meaningful scale- and shift-invariant loss를 설계하기 위해서는, prediction과 GT 값이 적절하게 scale, shift가 맞도록 align 해줘야 된다.
    
    - $s$는 scale estimator, $t$는 translation estimator 라고 할 때, 다음 두 식이 성립.
        ![](../source/스크린샷%202023-12-02%20오전%2012.22.28.png)
        
- 위의 alignment를 성립시키기 위해 두가지 접근법을 제시.
    
- 첫번째 접근법은 아래 식을 만족시키는 s,t를 구해서 aligned prediction을 뽑아내는 것. (Mean-squared errer, MSE)
![](../source/Pasted%20image%2020231202002219.png)
    MSE는 outlier에 취약하다는 단점.
    
- 두번째는 robust loss function.
![](../source/Pasted%20image%2020231202002210.png)
`**Final Loss.**`

- disparity space에 multi-scale, scale-invariant gradient matching term을 추가. 해당 term은 discontinuities에 대해 sharp하게 반응하고, GT와 같은 부분에 discontinuities가 있다는 bias.
    
![](../source/Pasted%20image%2020231202002200.png)

---

### Experiments

`**Comparison of loss functions`.**
![](../source/Pasted%20image%2020231202002154.png)
RW: ReDWeb

MD: MegaDepth

3D Movies: MV