
---
title:  "Towards Robust Monocular Depth Estimation Mixing Datasets forZero-shot Cross-dataset Transfer"

toc: true
toc_sticky: true
 
date: 2023-12-02 
last_modified_at: 2023-12-02

---
### Abstract

- monocular depth estimationì—ëŠ” large, diverse í›ˆë ¨ ë°ì´í„°ì…‹ì´ ì¤‘ìš”.
    
- Due to the **challenges** associated with acquiring dense ground-truth depth across different environments at scale, a number of datasets with distinct characteristics and biases have emerged. We develop tools that enable mixing multiple datasets during training, even if their annotations are incompatible.
    
- propose a robust training objective that is invariant to changes in depth range and scale, advocate the use of principled multi-objective learning to combine data from different sources, and highlight the importance of pretraining encoders on auxiliary tasks.
    
- To demonstrate the generalization power of our approach we use _zero-shot cross-dataset transfer._
    
    â‡’ mixing data from complementary sources greatly improves monocular depth estimation.
    

---

<aside> ğŸ«’ Mixing data from complementary sources greatly improves monocular depth estimation.

</aside>

### Introduction

`**Existing Problems**`

- none of the existing datasets is sufficiently rich to support the training of a model that works robustly on real images of diverse scenes.
- ê·¸ëŸ¬ë‚˜ ì„œë¡œ ë³´ì¶©ì´ ë  ë§Œí•œ ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ì´ ë‹¤ì–‘í•˜ê²Œ ì¡´ì¬í•˜ëŠ” ìƒí™©.

**`Summary`**

- We develop novel loss functions that are invariant to the major sources of incompatibility between datasets, including unknown and inconsistent scale and baselines.
- Diverse sensing modalitiesë“¤ë¡œë¶€í„° ì–»ì€ ë°ì´í„°(stereo cameras, laser scanners, structured light sensors)ë“¤ê¹Œì§€ë„ í•™ìŠµ ê°€ëŠ¥í•˜ë„ë¡ lossë¥¼ ì„¤ê³„. ë˜í•œ, monocular depth ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ ë°ì´í„°ì…‹ë“¤ì„ ì–´ë–»ê²Œ ê°€ì¥ ì´ìƒì ìœ¼ë¡œ í˜¼í•©í• ì§€ê¹Œì§€ ì—°êµ¬í–ˆìŒ. multi-objective optimizationì„ ê¸°ë°˜ìœ¼ë¡œí•œ ì ‘ê·¼ì´ naive mixing ì „ëµë³´ë‹¤ ì„±ê³¼ê°€ ì¢‹ì•˜ë‹¤.
- We further empirically highlight the importance of high-capacity encoders, and show the unreasonable effectiveness of pretraining the encoder on a large-scale auxiliary task.
- 6ê°œ ë°ì´í„°ì…‹ì—ì„œ quantitatively & qualitativelyí•˜ê²Œ ì†Œíƒ€ë¥¼ ëŠ¥ê°€í•˜ëŠ” ì„±ëŠ¥ ë‹¬ì„±.
![](../source/Pasted%20image%2020231202002334.png)
- disparity & depth
    
![](../source/Pasted%20image%2020231202002327.png)
    

---

### Existing Datasets & 3D MOVIES
![](../source/Pasted%20image%2020231202002315.png)
![](../source/Pasted%20image%2020231202002309.png)
`**challenges**`

**1)**

3D ì˜í™”ëŠ” ê´€ê°ìœ¼ë¡œ í•˜ì—¬ê¸ˆ í¸ì•ˆí•œ ì‹œì²­ì„ ì œê³µí•˜ê¸° ìœ„í•´ artistic, psychophysical(ì •ì‹ ë¬¼ë¦¬í•™ì )ì¸ ê³ ë ¤ì‚¬í•­ë“¤ ê¸°ë°˜ìœ¼ë¡œ disparity rangeë¥¼ ê²°ì •í•˜ëŠ” ê²½ìš°ê°€ ë§ë‹¤.

ì˜ˆë¥¼ ì˜í™”ì˜ ì‹œì‘ë¶€ë¶„ê³¼ ëë¶€ë¶„ì—ì„œëŠ” disparity rangeê°€ ì»¤ì§€ì§€ë§Œ, ì˜í™”ì˜ ì¤‘ë°˜ë¶€ì—ì„œëŠ” ì¢€ ë” í¸ì•ˆí•œ ì‹œì²­ì„ ìœ„í•´ disparity rangeë¥¼ ì¢ê²Œ ì¡°ì ˆí•œë‹¤.

â†’ â€œfocal lengths, baseline, convergence angle between the cameras of the stereo rigâ€ ê°€ í•˜ë‚˜ì˜ ì˜í™” ì•ˆì—ì„œë„ ë§¤ìš° ë‹¬ë¼ì§€ê²Œ ëœë‹¤.

**2)**

ë³´í†µì˜ stereo cameraì™€ëŠ” ë‹¤ë¥´ê²Œ, ì˜í™”ì—ì„œ ì‚¬ìš©ë˜ëŠ” stereo pairsë“¤ì€ positive, negative disparitiesë¥¼ í¬í•¨í•œë‹¤. (screenë³´ë‹¤ ì•ì— ìˆëŠ”ì§€ ë’¤ì— ìˆëŠ”ì§€ í‘œí˜„í•˜ê¸° ìœ„í•œ-)

**3)**

post-productionìœ¼ë¡œ image pairsë“¤ì„ shifting ì²˜ë¦¬í•˜ê¸°ë„ í•¨..

â‡’ ìœ„ ë¬¸ì œë“¤ì„ í•´ê²°í•˜ê¸° ìœ„í•´ data extraction & training procedures.

`**Movie Selection and Preprocessing**`

- 23 í¸ì˜ ì˜í™” ì„ ì •, ë‹¤ìŒê³¼ ê°™ì€ ê¸°ì¤€ ì ìš©
    
    1. We only selected movies that were shot using a physical stereo camera. (Some 3D films are shot with a monocular camera and the stereoscopic effect is added in post-production by artists.)
        
    2. We tried to balance realism and diversity.
        
    3. We only selected movies that are available in Blu-ray format and thus allow extraction of high-resolution images.
        
- 1920x1080 í•´ìƒë„, 24 fps(frames per second)ë¡œ ì¶”ì¶œ â†’ ì˜í™”ë§ˆë‹¤ ì¥ë©´ í¬ê¸°ê°€ ë‹¬ë¼ black barê°€ ë°œìƒí•˜ëŠ” ê²½ìš°ê°€ ìˆì—ˆìŒ. ë”°ë¼ì„œ 1880x800 ì‚¬ì´ì¦ˆë¡œ center crop.
    

`**Disparity extraction**`

- stereo matchingì„ ì‚¬ìš©í•´ì„œ pairsê°„ì˜ disparity mapsì„ ë½‘ì•„ë‚´ë ¤ê³  í–ˆì§€ë§Œ, SOTA stereo matcherë“¤ë„ movie dataì—ì„œëŠ” ì¢‹ì§€ ì•Šì€ ê²°ê³¼ë¥¼ ë³´ì„. ì´ëŠ” stereo matcherê°€ positive disparity rangesë¼ëŠ” ê°€ì •í•˜ì—ì„œ í•™ìŠµë˜ì—ˆê¸° ë•Œë¬¸. (standard stereo cameraì—ê²ŒëŠ” ì í•©í•œ ê°€ì •ì´ì§€ë§Œ, stereoscopic filmì—ì„œëŠ” ë“¤ì–´ë§ì§€ ì•ŠëŠ”ë‹¤.)
- ê¸°ì¡´ì˜ ë°ì´í„°ë“¤ì— ë¹„í•´, 3D ì˜í™”ì˜ disparity rangeê°€ ë§¤ìš° ì‘ë‹¤(due to the limited depth budget)ëŠ” ì ë„ ë¬¸ì œ.

â‡’ optical flow algorithmì„ ì ìš©í•´ì„œ ë¬¸ì œ í•´ê²°.

- Optical flow algorithms naturally handle both positive and negative disparities and usually perform well for displacements of moderate size.
- stereo pairì—ì„œ left cameraë¥¼ referenceë¡œ ë‘ê³ , optical flowë¥¼ ì¶”ì¶œ. ì´í›„ì— right cameraë¥¼ referenceë¡œ ë‘ê³  optical flowë¥¼ ë‹¤ì‹œ ì¶”ì¶œí•´ ì¤€ ë‹¤ìŒì—, left-right consistencyë¥¼ ì²´í¬ â†’ bad disparity qualityë¥¼ ê°–ëŠ” frameì€ filter out ê°€ëŠ¥!
- pretrained semantic segmentation ëª¨ë¸ë¡œ í•˜ëŠ˜ ì°¾ì•„ë‚´ê³ , minimum disparity in the imageë¡œ ì„¸íŒ…í•´ì¤€ë‹¤.

---

### Training on Diverse Data

`**challenges**`

1. **Inherently different representations of depth**: direct vs. inverse depth representations.
    
2. **Scale ambiguity**: for some data sources, depth is only given up to an unknown scale.
    
3. **Shift ambiguity**: some datasets provide disparity only up to an unknown scale and global disparity shift that is a function of the unknown baseline and a horizontal shift of the principal points due to post-processing
    

â†’ The main requirement for a sensible training scheme is to carry out computations in an appropriate output space that is compatible with all ground-truth representations and is numerically well- behaved and to design a loss function that is flexible enough to handle diverse sources of data.

**`Scale- and shift-invariant losses.`**

- We propose to perform pre- diction in disparity space (inverse depth up to scale and shift) together with a family of scale- and shift-invariant dense losses to handle the aforementioned ambiguities.
![](../source/Pasted%20image%2020231202002249.png)
- meaningful scale- and shift-invariant lossë¥¼ ì„¤ê³„í•˜ê¸° ìœ„í•´ì„œëŠ”, predictionê³¼ GT ê°’ì´ ì ì ˆí•˜ê²Œ scale, shiftê°€ ë§ë„ë¡ align í•´ì¤˜ì•¼ ëœë‹¤.
    
    - $s$ëŠ” scale estimator, $t$ëŠ” translation estimator ë¼ê³  í•  ë•Œ, ë‹¤ìŒ ë‘ ì‹ì´ ì„±ë¦½.
        ![](../source/ìŠ¤í¬ë¦°ìƒ·%202023-12-02%20ì˜¤ì „%2012.22.28.png)
        
- ìœ„ì˜ alignmentë¥¼ ì„±ë¦½ì‹œí‚¤ê¸° ìœ„í•´ ë‘ê°€ì§€ ì ‘ê·¼ë²•ì„ ì œì‹œ.
    
- ì²«ë²ˆì§¸ ì ‘ê·¼ë²•ì€ ì•„ë˜ ì‹ì„ ë§Œì¡±ì‹œí‚¤ëŠ” s,të¥¼ êµ¬í•´ì„œ aligned predictionì„ ë½‘ì•„ë‚´ëŠ” ê²ƒ. (Mean-squared errer, MSE)
![](../source/Pasted%20image%2020231202002219.png)
    MSEëŠ” outlierì— ì·¨ì•½í•˜ë‹¤ëŠ” ë‹¨ì .
    
- ë‘ë²ˆì§¸ëŠ” robust loss function.
![](../source/Pasted%20image%2020231202002210.png)
`**Final Loss.**`

- disparity spaceì— multi-scale, scale-invariant gradient matching termì„ ì¶”ê°€. í•´ë‹¹ termì€ discontinuitiesì— ëŒ€í•´ sharpí•˜ê²Œ ë°˜ì‘í•˜ê³ , GTì™€ ê°™ì€ ë¶€ë¶„ì— discontinuitiesê°€ ìˆë‹¤ëŠ” bias.
    
![](../source/Pasted%20image%2020231202002200.png)

---

### Experiments

`**Comparison of loss functions`.**
![](../source/Pasted%20image%2020231202002154.png)
RW: ReDWeb

MD: MegaDepth

3D Movies: MV